{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from textblob_de import TextBlobDE as TextBlob\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import nltk\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funktion um Auswirkungen der Preprocessing Schritte zu untersuchen\n",
    "def count_words(data, message=\"\"):\n",
    "    _ = [word_tokenize(x) for x in data]\n",
    "    unique_words = len(set(list(chain.from_iterable(_))))\n",
    "    total_words = len(list(chain.from_iterable(_)))\n",
    "    print(\"Step: {}\".format(message))\n",
    "    print(\"Count unique Words: {}\".format(unique_words))\n",
    "    print(\"Count Words total: {}\".format(total_words))\n",
    "    print()\n",
    "    \n",
    "# regex welche für die Negationswörter abdeckt\n",
    "negation_regex = \"nicht[s]?|kein|keine[rsmn]|nie|niemals|ohne|kaum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# einlesen des Datensatzes\n",
    "thomann_data = pd.read_csv(\"thomann_reviews_all.csv\")\n",
    "thomann_data[\"review_old\"] = thomann_data[\"review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wörter werden gleich zu Beginn gezählt, da späteres Negationshandling die originale Satzstruktur benötigt \n",
    "# aber die Auswirkungen auf den Korpus dennoch ausgegeben werden sollen\n",
    "\n",
    "# keine Sonderzeichen zählen\n",
    "baseline = [re.sub(\"[^\\w]\",\" \", rev) for rev in thomann_data[\"review\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging um das Negationshandling zu ermöglichen\n",
    "thomann_data[\"taggedWords\"] = [TextBlob(review).tags for review in thomann_data[\"review\"]]\n",
    "\n",
    "def identify_negations_bigram(tagged_review_words):\n",
    "    word_tag_pairs = nltk.bigrams(tagged_review_words)\n",
    "    # findet Adjektive oder Nomen innerhalb der Reviewtags, welche Negation vorangestellt haben und gibt diese Wort-Tupel zurück\n",
    "    matched_pairs = [(a[0], b[0]) for (a, b) in word_tag_pairs if re.match(negation_regex, a[0],flags=re.IGNORECASE) \n",
    "                                                                 and (re.match(\"JJ*\",b[1]) or re.match(\"NN*\",b[1])) \n",
    "                                                                 and re.match(\"[\\w]+\",b[0])]\n",
    "    # instanziiere Hilfslisten\n",
    "    search_list = list()\n",
    "    replacement_list = list()\n",
    "    # für alle gefunden Tupel \n",
    "    for pair in matched_pairs:\n",
    "        # konvertiere Tupel zu String \n",
    "        search_string = ' '.join(pair)\n",
    "        # hänge an das zweite Wort NOT_\n",
    "        replacement = search_string.split()[0] +\" NOT_\"+ search_string.split()[1]\n",
    "        # hänge Ergebnisse an Liste\n",
    "        search_list.append(search_string)\n",
    "        replacement_list.append(replacement)\n",
    "    # übergebe gezippte Liste\n",
    "    return list(zip(search_list,replacement_list))\n",
    "# speichere identifizierte negations Bigrams in neuer Spalte\n",
    "thomann_data[\"replace_negations_bigram\"] = thomann_data[\"taggedWords\"].apply(identify_negations_bigram)\n",
    "\n",
    "def identify_negations_trigram(tagged_review_words):\n",
    "    word_tag_tris = nltk.trigrams(tagged_review_words)\n",
    "    matched_tris = [(a[0],b[0],c[0]) for (a,b,c) in word_tag_tris if re.match(negation_regex, a[0],flags=re.IGNORECASE) \n",
    "                                                                    and re.match(\"RB*\", b[1]) \n",
    "                                                                    and (re.match(\"JJ*\",c[1]) or re.match(\"NN*\",c[1])) \n",
    "                                                                    and re.match(\"[\\w]+\",c[0])]\n",
    "    # instanziiere Hilfslisten\n",
    "    search_list = list()\n",
    "    replacement_list = list()\n",
    "    for pair in matched_tris:\n",
    "        # konvertiere Tupel zu String \n",
    "        search_string = ' '.join(pair)\n",
    "        # hänge an das zweite Wort NOT_\n",
    "        replacement = search_string.split()[0] + \" \" + search_string.split()[1] +\" NOT_\"+ search_string.split()[2]\n",
    "        # hänge Ergebnisse an Liste\n",
    "        search_list.append(search_string)\n",
    "        replacement_list.append(replacement)\n",
    "    # übergebe gezippte Liste\n",
    "    return list(zip(search_list,replacement_list))\n",
    "# speichere identifizierte negations Trigrams in neuer Spalte\n",
    "thomann_data[\"replace_negations_trigram\"] = thomann_data[\"taggedWords\"].apply(identify_negations_trigram)\n",
    "\n",
    "# Merge der beiden Ersetzungen\n",
    "thomann_data[\"replace_negations\"] = thomann_data[\"replace_negations_trigram\"] + thomann_data[\"replace_negations_bigram\"]\n",
    "#thomann_data.drop([\"replace_negations_trigram\",\"replace_negations_bigram\"], inplace = True, axis = 1)\n",
    "\n",
    "# ersetzte die neuen NOT Wörter im Review Text\n",
    "def replacement(x):\n",
    "    review = x[\"review\"]\n",
    "    for t in x[\"replace_negations\"]:\n",
    "        review = re.sub(t[0], t[1], review)\n",
    "    return review\n",
    "thomann_data[\"review\"] = thomann_data.apply(replacement, axis = 1)\n",
    "thomann_data.drop([\"replace_negations\"], inplace = True, axis = 1)\n",
    "\n",
    "# nun sind die Tags zwar bei der Spalte richtig aber die getaggten Tupel sind noch falsch\n",
    "# es erfolgt ein Abgleich zwischen der alten spalte \"taggedWords\" (hier sind die POS-Tags noch richtig) und der neuen Spalte\n",
    "# \"taggedWords_neg\" (hier sind die Wörter richtig z.B. not_gut)\n",
    "thomann_data[\"taggedWords_neg\"] = [TextBlob(review).tags for review in thomann_data[\"review\"]]\n",
    "\n",
    "def choose_right_POS(x):\n",
    "    # words = z[0][0]\n",
    "    # tags = z[1][1]\n",
    "    list_of_tupels = list()\n",
    "    for z in zip(x[\"taggedWords_neg\"],x[\"taggedWords\"]):\n",
    "        list_of_tupels.append((z[0][0],z[1][1]))\n",
    "    return list_of_tupels\n",
    "thomann_data[\"taggedWords_neg\"] = thomann_data.apply(choose_right_POS, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING mit Beachtung der Satzstruktur, ist nur dazu da, dass tokenSent_pp_neg erstellt werden kann (wird für W2V training benötigt)\n",
    "for mode in [\"review\",\"review_old\"]:\n",
    "\n",
    "    thomann_data[\"review_temp\"] = [re.sub(\"[^\\w\\.\\!\\?]\",\" \", rev) for rev in thomann_data[mode]]\n",
    "    thomann_data[\"review_temp\"] = [rev.lower() for rev in thomann_data[\"review_temp\"]]\n",
    "    thomann_data[\"review_temp\"] = [re.sub(\"[0-9]+\",\" \", rev) for rev in thomann_data[\"review_temp\"]]\n",
    "    thomann_data[\"review_temp\"] = [re.sub(\"[ \\t]+ \",\" \", rev) for rev in thomann_data[\"review_temp\"]]\n",
    "\n",
    "    # stopwords\n",
    "    stop = stopwords.words('german')\n",
    "    # extend \n",
    "    stop.append(\"dass\")\n",
    "    thomann_data[\"review_temp\"] = thomann_data[\"review_temp\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "    # stemming \n",
    "    stemmer = SnowballStemmer(\"german\")\n",
    "    def stem(review):\n",
    "        all_words = review.split()\n",
    "        all_words_stemmed = [stemmer.stem(x) for x in all_words]\n",
    "        return \" \".join(all_words_stemmed)\n",
    "    thomann_data[\"review_temp\"] = thomann_data[\"review_temp\"].apply(stem)\n",
    "    \n",
    "    # wenn es sich um rohe reviews handelt erstelle die preprocess only Reviewtext\n",
    "    if mode == \"review_old\":\n",
    "        thomann_data[\"review_pp_only\"] = thomann_data[\"review_temp\"]\n",
    "    \n",
    "    # wenn es sich um not_ reviews handelt erstelle tokenSent\n",
    "    if mode == \"review\":\n",
    "        thomann_data[\"tokenSent_pp_neg\"] = [sent_tokenize(review) for review in thomann_data[\"review_temp\"]]\n",
    "\n",
    "    thomann_data.drop([\"review_temp\"], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: Ausgangssituation\n",
      "Count unique Words: 69305\n",
      "Count Words total: 2183298\n",
      "\n",
      "Step: nach NOT\n",
      "Count unique Words: 72869\n",
      "Count Words total: 2183298\n",
      "\n",
      "Step: nach Case Insensitive\n",
      "Count unique Words: 63310\n",
      "Count Words total: 2183298\n",
      "\n",
      "Step: ohne Zahlen\n",
      "Count unique Words: 60777\n",
      "Count Words total: 2163233\n",
      "\n",
      "Step: ohne Stoppwörter\n",
      "Count unique Words: 60550\n",
      "Count Words total: 1116581\n",
      "\n",
      "Step: nach Stemmer\n",
      "Count unique Words: 43308\n",
      "Count Words total: 1116581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING NORMAL\n",
    "count_words(baseline, \"Ausgangssituation\")\n",
    "# entferne alles was kein buchstabe oder zahl ist\n",
    "thomann_data[\"review\"] = [re.sub(\"[^\\w]\",\" \", rev) for rev in thomann_data[\"review\"]]\n",
    "\n",
    "test = [re.sub(\"[^\\w]\",\" \", rev) for rev in thomann_data[\"review\"]]\n",
    "\n",
    "count_words(thomann_data[\"review\"], \"nach NOT\")\n",
    "thomann_data[\"review\"] = [rev.lower() for rev in thomann_data[\"review\"]]\n",
    "count_words(thomann_data[\"review\"], \"nach Case Insensitive\")\n",
    "thomann_data[\"review\"] = [re.sub(\"[0-9]+\",\" \", rev) for rev in thomann_data[\"review\"]]\n",
    "count_words(thomann_data[\"review\"], \"ohne Zahlen\")\n",
    "thomann_data[\"review\"] = [re.sub(\"[ \\t]+ \",\" \", rev) for rev in thomann_data[\"review\"]]\n",
    "\n",
    "# stopwords\n",
    "stop = stopwords.words('german')\n",
    "# extend \n",
    "stop.append(\"dass\")\n",
    "thomann_data[\"review\"] = thomann_data[\"review\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "count_words(thomann_data[\"review\"], \"ohne Stoppwörter\")\n",
    "# stemming \n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "def stem(review):\n",
    "    all_words = review.split()\n",
    "    all_words_stemmed = [stemmer.stem(x) for x in all_words]\n",
    "    return \" \".join(all_words_stemmed)\n",
    "thomann_data[\"review\"] = thomann_data[\"review\"].apply(stem)\n",
    "thomann_data[\"tokenWord_pp_neg\"] = [word_tokenize(review) for review in thomann_data[\"review\"]]\n",
    "\n",
    "count_words(thomann_data[\"review\"], \"nach Stemmer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('enriched_thomann_data.pickle', 'wb') as f:\n",
    "        pickle.dump(thomann_data, f, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
